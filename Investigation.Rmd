# Code for investigating pretrained nGram Models

```{r}
library(wordVectors)
library(magrittr)
modelname = "INSERT FILENAME HERE"
```

```{r}
model = read.binary.vectors(modelname)
```

1. The `vectors` parameter is the dimensionality of the representation. More vectors usually means more precision, but also more random error and slower operations. Likely choices are probably in the range 100-500.
2. The `threads` parameter is the number of processors to use on your computer. On a modern laptop, the fastest results will probably be between 2 and 8 threads, depending on the number of cores.
3. `iter` is how many times to read through the corpus. With fewer than 100 books, it can greatly help to increase the number of passes; if you're working with billions of words, it probably matters less. One danger of too low a number of iterations is that words that aren't closely related will seem to be closer than they are.
4. More training iterations reduces noise, so don't be afraid to set things up to require a lot of training time (e.g. a day).
5. One of the best things about the word2vec algorithm is that it *does* work on extremely large corpora in linear time.
6. In RStudio I've noticed that this sometimes appears to hang after a while; the percentage bar stops updating. If you check system activity it actually is still running, and will complete.
7. If at any point you want to *read in* a previously trained model, you can do so by typing `model =  read.vectors("cookbook_vectors.bin")`.

Now we have a model in memory. What can it tell us?

## Similarity searches -- you can run some basic operations to find the nearest elements:

```{r}
model %>% closest_to("nazi ")
```

With that list, you can expand out further to search for multiple words:

```{r}
model %>% 
  closest_to(model[[c("nazi","hitler","germany","jew","war")]],50)

df<-as.data.frame(model %>% 
closest_to(model[[c("climate_change","climate_action","global_warming")]],50))
#write.csv(df,file='cc_50_terms.csv',row.names = FALSE)

df<-as.data.frame(model %>% 
closest_to(model[[c("carbon_neutral",
 "carbon_offset",
 "carbon_reduction",
 "carbon_capture",
 "carbon_sink",
 "co2_emissions",
 "decarbonization",
 "emissions",
 "environmental_footprint",
 "greenhouse_gas",
 "cleaner_energy",
 "carbon_footprint",
 "carbon_intensity",
 "carbon_emission",
 "carbon_emissions",
 "carbon_reduction",
 "carbon_sequestration",
 "environmental_footprint",
 "greenhouse_gas",
 "greenhouse_gases",
 "low_carbon",
 "emisison_free",
 "energy_efficiency")]],300))
write.csv(df,file='emissions_300_terms.csv',row.names = FALSE)
```

#emerging themes: politics, renewables, but other interesting terms: avert, society
Now we have a pretty expansive list of cc-related words. This can be useful for a few different things:

1. As a list of potential query terms for keyword search.
2. As a batch of words to use as seed to some other text mining operation; for example, you could pull all paragraphs surrounding these to find ways that the terms are used.
3. As a source for visualization.

## Clustering

We can use standard clustering algorithms, like kmeans, to find groups of terms that fit together. You can think of this as a sort of topic model, although unlike more sophisticated topic modeling algorithms like Latent Direchlet Allocation, each word must be tied to a single particular topic.

```{r}
set.seed(10)
centers = 150
clustering = kmeans(model,centers=centers,iter.max = 40)
```

Here are a ten random "topics" produced through this method. Each of the columns are the ten most frequent words in one random cluster.

```{r}
sapply(sample(1:centers,10),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:10])
})
```

These can be useful for figuring out, at a glance, what some of the overall common clusters in your corpus are.

Clusters need not be derived at the level of the full model. We can take, for instance, 
the 20 words closest to specified words.

```{r}
ingredients = c("climate_change","global_warming","climactic_change")
term_set = lapply(ingredients, 
       function(ingredient) {
          nearest_words = model %>% closest_to(model[[ingredient]],20)
          nearest_words$word
        }) %>% unlist

subset = model[[term_set,average=F]]

subset %>%
  cosineDist(subset) %>% 
  as.dist %>%
  hclust %>%
  plot

```

# Visualization - Relationship planes.

One of the basic strategies you can take is to try to project the high-dimensional space here into a plane you can look at. E.g. we can take the words "sweet" and "sour," find the twenty words most similar to either of them, and plot those in a sweet-salty plane.

(I can do this with the concs, see if they are closer to renewables or politix.)

```{r}
tastes = model[[c("global_warming","environment"),average=F]]

#model[1:3000,] here restricts to the 3000 most common words in the set.
sweet_and_saltiness = model[1:3000,] %>% cosineSimilarity(tastes)

#Filter to the top 20 sweet or salty.
sweet_and_saltiness = sweet_and_saltiness[
  rank(-sweet_and_saltiness[,1])<20 |
  rank(-sweet_and_saltiness[,2])<20,
  ]

plot(sweet_and_saltiness,type='n')
text(sweet_and_saltiness,labels=rownames(sweet_and_saltiness))

```

There's no limit to how complicated this can get. For instance, there are really *five* tastes: sweet, salty, bitter, sour, and savory.

Rather than use a base matrix of the whole set, we can shrink down to just five dimensions: how similar every word in our set is to each of these five. (I'm using cosine similarity here, so the closer a number is to 1, the more similar it is.)

```{r}

tastes = model[[c("global_warming","renewables","environmental_policies"),average=F]]

# model[1:3000,] here restricts to the 3000 most common words in the set.
common_similarities_tastes = model[1:3000,] %>% cosineSimilarity(tastes)

common_similarities_tastes[20:30,]
```

Now we can filter down to the 50 words that are closest to *any* of these (that's what the apply-max function below does), and we can use a PCA biplot to look at just 50 words in a flavor plane.

```{r}
high_similarities_to_tastes = common_similarities_tastes[rank(-apply(common_similarities_tastes,1,max)) < 75,]

high_similarities_to_tastes %>% 
  prcomp %>% 
  biplot(main="Fifty words in a\nprojection of flavor space")
```

This tells us a few things. One is that (in some runnings of the model, at least--there is some random chance built in here.) "sweet" and "sour" are closely aligned. Is this a unique feature of American cooking? A relationship that changes over time? These would require more investigation.

Second is that "savory" really is an acting category in these cookbooks, even without the precision of 'umami' as a word to express it. Anchovy, the flavor most closely associated with savoriness, shows up as fairly characteristic of the flavor, along with a variety of herbs.

Finally, words characteristic of meals seem to show up in the upper realms of the file.

# Catchall reduction: TSNE

Last but not least, there is a catchall method built into the library to visualize a single overall decent plane for viewing the library; TSNE dimensionality reduction.

Just calling "plot" will display the equivalent of a word cloud with individual tokens grouped relatively close to each other based on their proximity in the higher dimensional space. "Perplexity" is the optimal number of neighbors for each word. By default it's 50; smaller numbers may cause clusters to appear more dramatically at the cost of overall coherence.

```{r}
plot(model,perplexity=50)
```

A few notes on this method:

1. If you don't get local clusters, it is not working. You might need to reduce the perplexity so that clusters are smaller; or you might not have good local similarities.
2. If you're plotting only a small set of words, you're better off trying to plot a `VectorSpaceModel` with `method="pca"`, which locates the points using principal components analysis.

## http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html

```{r}
model %>% nearest_to(model[["climate_change"]]) %>% round(3)
model %>% nearest_to(model[[c("climate_change","global_warming")]],20) %>% names
```

The easiest way to explore these spaces is just to work iteratively until you get a vector that seems good enough. I’ll add some of the words I like from the bottom half of that search to get us towards something even more food-y. I’m doing this a few times, until we get out to about 300 words: the list below just shows 50 random words out of that set.

```{r}
cc_words = model %>% nearest_to(model[[c("climate_change","global_warming","clean_energy")]],300) %>% names
sample(cc_words,50)
```

Now we have a bunch of cc words typical of the data. We can use similarity scores for all sorts of things. We can, for instance, create a hierarchy of cc words based on their distances from each other.

```{r}
cc_terms = model[rownames(model) %in% cc_words[1:50],] #cc_terms = foods
cc_distances = cosineDist(cc_terms,cc_terms) %>% as.dist

plot(as.dendrogram(hclust(cc_distances)),horiz=F,cex=1,main="Cluster dendrogram of the fifty words closest to a cc vector\nin SeekingAlpha data")
```

We can also pull out similarity scores for all words in the set to either of two terms: a meat vector and a vegetable vector: this lets us plot a number of food terms in a discursive space vaguely defined as “meatness” and “vegetableness.” 

```{r}
cc_terms = model[rownames(model) %in% cc_words,]
cost_score = cc_terms %>% cosineSimilarity(model[[c("cost","expense")]])
gain_score = cc_terms %>% cosineSimilarity(model[[c("opportunity","potential")]])
plot(cost_score,gain_score,type='n',main="Top 300 cc words plotted by their similarity to costs\n(x axis) and gains/opportunities (y axis).")
text(cost_score,gain_score,labels=rownames(cc_terms),cex=.7)
abline(a=0,b=1)

```

We don’t need two dimensions to capture the meat-vegetable relationship: we can do it in one. Just as “meat” is a vector and “vegetable” is a vector, “meat”-“vegetable” is also something that we can track in space. In the text_vectors package, we can simply indicate this by comparing our words to a new vector defined as model[["cost"]] - model[["gain"]]

This is a relationship, not a vocabulary item: but it is defined in the same vector space, so we can score any words based on their relationships to it. This means we can create text layouts specific to any desired word relationship. If that relationship is so important that it makes its way into the model’s outputs, we can see it in action.

```{r}
all_cc_terms = data.frame(word = rownames(cc_terms))
cost_gain_vector = model[[c("cost","expense","bad")]] - model[[c("gain","opportunity","good")]]
all_cc_terms$cost_vs_gain = cosineSimilarity(cc_terms,cost_gain_vector)

risk_vector = model[[c("regulation","policy")]] - model[[c("deregulation","trump")]]
all_cc_terms$policy_risk_vs_existential_risk = cosineSimilarity(cc_terms,risk_vector)

library(ggplot2)
ggplot(all_cc_terms,aes(x=cost_vs_gain,y=policy_risk_vs_existential_risk,label=word)) + geom_text(size=2.5) +
  scale_y_continuous("<----- deregulation ..............  regulation ------>",limits=c(-.45,.25)) +
  scale_x_continuous("<----- more gain ..............  more cost ------>",limits=c(-.25,.33))
```

  
